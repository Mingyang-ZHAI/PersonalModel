version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    container_name: personalization-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
      - ./models:/models
    environment:
      - OLLAMA_ORIGINS=*
      - OLLAMA_HOST=0.0.0.0
    restart: unless-stopped
    # If you have an NVIDIA GPU, uncomment the lines below
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]
    
  webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: personalization-webui
    ports:
      - "8080:8080"
    volumes:
      - webui-data:/app/backend/data
      - ./preference.md:/app/backend/data/preference.md:ro
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - WEBUI_AUTH=False
      - DEFAULT_MODELS=qwen3:4b
      - ENABLE_SIGNUP=True
      - WEBUI_NAME=Personalized LLM Assistant
      # Set default System Prompt from preference.md file
      - DEFAULT_SYSTEM_PROMPT_FILE=/app/backend/data/preference.md
    depends_on:
      - ollama
    restart: unless-stopped

  # Automatic model download service (first run)
  model-downloader:
    image: ollama/ollama:latest
    container_name: model-downloader
    depends_on:
      - ollama
    environment:
      - OLLAMA_HOST=http://ollama:11434
    command: >
      sh -c "
        echo 'Waiting for Ollama service to start...' &&
        sleep 30 &&
        echo 'Starting model download...' &&
        ollama pull qwen3:4b &&
        echo 'Model download completed!'
      "
    restart: "no"

volumes:
  ollama-data:
  webui-data:

networks:
  default:
    name: personalization-network